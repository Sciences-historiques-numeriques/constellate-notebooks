{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a9357d7-7721-4d80-8533-37241c93753e",
   "metadata": {},
   "source": [
    "<img align=\"left\" src=\"https://ithaka-labs.s3.amazonaws.com/static-files/images/tdm/tdmdocs/CC_BY.png\"><br />\n",
    "\n",
    "Created by [Nathan Kelber](http://nkelber.com) under [Creative Commons CC BY License](https://creativecommons.org/licenses/by/4.0/)<br />\n",
    "For questions/comments/improvements, email nathan.kelber@ithaka.org.<br />\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e77886-9fb4-4afc-948b-0191b4941272",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Language Models 1\n",
    "\n",
    "**Description:** This lesson offers some examples of language models, giving a basic outline of concepts such as:\n",
    "\n",
    "* Historical Approaches to NLP\n",
    "* Word embeddings\n",
    "* Transformers\n",
    "\n",
    "Learners will use the Gensim and 🤗 Transformers library to explore aspects of language models including:\n",
    "\n",
    "* Word Vectors\n",
    "* Text Generation\n",
    "* Sentiment Analysis\n",
    "* Named Entity Recognition\n",
    "* Question Answering\n",
    "* Summarization\n",
    "\n",
    "**Use Case:** For Learners (Detailed explanation, not ideal for researchers)\n",
    "\n",
    "**Difficulty:** Intermediate\n",
    "\n",
    "**Completion Time:** 75 minutes\n",
    "\n",
    "**Knowledge Required:** \n",
    "* Python Basics\n",
    "* Pandas Basics\n",
    "\n",
    "**Knowledge Recommended:** \n",
    "* Python Intermediate\n",
    "* Pandas Intermediate\n",
    "* A Basic Grasp of Neural Networks\n",
    "\n",
    "**Data Format:** None\n",
    "\n",
    "**Libraries Used:** \n",
    "* [🤗 Transformers](https://huggingface.co/docs/transformers/index)- provides APIs and tools to easily download and train pretrained models\n",
    "* [Pytorch](https://pytorch.org/)- a popular machine learning framework\n",
    "* [xFormers](https://github.com/facebookresearch/xformers)- for improving transformer computation speed\n",
    "\n",
    "**Research Pipeline:** None\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3353d4d-022a-4bdc-a990-af4c22f160ae",
   "metadata": {},
   "source": [
    "<h3 style=\"color:red; display:inline\">Note:</h3>\n",
    "\n",
    "Language models and datasets come in many sizes. The models and datasets for this notebook were tested on the given tasks, but for other models/tasks it is a good idea to check the file size and requirements. If you load or use a language model that is too big, you may fill all of the available space (10 GB) and/or memory (8 GB) in your lab. If the memory is full, try restarting the kernel (or restarting the lab). If the disk space is full, before deleting your own files, delete the .cache directory to clear out downloaded datasets and models from your space. You can do this by running the following code cell:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e07fe65c-f54e-423c-b35d-bbcc60a5cac9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the .cache folder\n",
    "!rm -r /home/jovyan/.cache/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbcc6e99-1b0e-4001-9a0f-d7459cf5cc82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check current disk space usage\n",
    "!df -h /home/jovyan/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "390255f9-950a-4bab-b0a5-cfe90597ff22",
   "metadata": {},
   "source": [
    "If you are familiar with the command line, you can use a terminal session to remove individual models and datasets. 🤗 Hugging Face stores them in the following places.\n",
    "\n",
    "**Datasets**\n",
    "```~/.cache/huggingface/datasets```\n",
    "\n",
    "**Models**\n",
    "```~/.cache/hub```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6df0f886-4bec-4118-82b7-2948a0ffd124",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "500c1bf6-8a19-4a20-ac39-c342ccedc47c",
   "metadata": {},
   "source": [
    "# Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca2db38d-5dce-4860-8036-286a6f7f5ccc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install 🤗 Transformers\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47850e62-ae76-41de-94e1-39c50ef15348",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Xformers\n",
    "!pip install xformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3235be26-6532-4494-aad9-ffc265707478",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install Sentencepiece, a  a subword tokenizer and detokenizer for natural language processing\n",
    "# that uses byte-pair-encoding (BPE)\n",
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d69c3796-254e-4846-9b7f-d5750a07514a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install sacremoses, a Python port of the Moses tokenizer\n",
    "!pip install sacremoses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ac447-f1e2-46fd-8560-6f22ee87eeb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Install datasets, a library for working with dataset files from 🤗 Hugging Face \n",
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22d2d815-cceb-4cd8-a885-260d441ba401",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1a19ebe-f810-4262-9d51-075a2091c312",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, set_seed\n",
    "import pandas as pd\n",
    "from datasets import load_dataset_builder\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2353418b-27b4-47c8-b6d9-8b20a8929f42",
   "metadata": {},
   "source": [
    "## Loading a 🤗 Hugging Face dataset \n",
    "The datasets library can help us view information about datasets and download them from the 🤗 Hugging Face repository. Datasets can be very large, so it is a good idea to do the following before trying to load the whole dataset:\n",
    "\n",
    "* Check the dataset information on the 🤗 Hugging Face website to get a sense of the file size.\n",
    "* Use `load_dataset_builder` before `load_dataset` to view the dataset description and features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e4a6bdf-389d-46ce-aa82-8d819d9047b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# We can grab a particular dataset builder\n",
    "# without downloading the whole dataset\n",
    "# That allows us to preview its description and features first\n",
    "# Dataset https://huggingface.co/datasets/gsm8k\n",
    "ds_builder = load_dataset_builder(\"gsm8k\", 'main')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c07851-06af-4f1c-9cef-bf721679cacb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use .info.description to retrieve the description\n",
    "ds_builder.info.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c2f8bc3-6274-4a95-b30a-9a9a76bea40a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Use .info.features to retrieve the features\n",
    "ds_builder.info.features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375074d5-349e-483a-bd54-ee93c56d089b",
   "metadata": {},
   "source": [
    "## Dataset Categories and Splits\n",
    "\n",
    "Large datasets often come in a variety of configurations and/or splits. A configuration, for example, might be the particular language in a multilingual dataset. A split is usually part of a machine learning workflow, i.e. \"train\", \"validation\", \"test\".\n",
    "\n",
    "This information can usually be found on the 🤗 Hugging Face page for the dataset, usually under the \"dataset card\" or in the `README.md` under \"Files and versions\". You can also try using `load_dataset_builder` without a second argument and the resulting error may list the configurations/splits options.\n",
    "\n",
    "Let's load the `1510_1699` configuration of `TheBritishLibrary/blbooks`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "038467f9-4b1f-43d8-a1bd-a45c8b2314a5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading a dataset\n",
    "# https://huggingface.co/datasets/TheBritishLibrary/blbooks\n",
    "dataset = load_dataset(\"TheBritishLibrary/blbooks\", '1510_1699')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9542e66a-846c-4c27-b2d8-cc62989b6d56",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# The dataset structure is similar to a Python dictionary\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2f423d1-d0b8-4ab1-b2ad-79d9a408ddb8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Select the train split\n",
    "train_ds = dataset[\"train\"]\n",
    "train_ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd55985-73e3-4027-a89b-4f72abef7887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Find the number of items in our split\n",
    "len(train_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7feb07e0-3746-49dc-9253-5744d2321fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine an individual record using an index\n",
    "train_ds[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1f5d64-2a50-4efd-aa71-6c9e05de76b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine the dataset columns/features\n",
    "train_ds.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4812931-ea83-4a23-b2b5-3ea5a34a3051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine the dataset columns/features\n",
    "train_ds.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ffcaf91-2330-41df-beff-d0773108fc20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Preview a particular column/feature\n",
    "train_ds[\"title\"][:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f434ed05-a44f-421a-8b96-b02090db55c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Examine the dataset as a Pandas dataframe\n",
    "dataset.set_format(type=\"pandas\")\n",
    "df = dataset[\"train\"][:]\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e5a46a3-1201-4e4a-bfcd-9da613be8b4a",
   "metadata": {},
   "source": [
    "# Text Generation\n",
    "By default, the 🤗 Transformers library text generation pipeline uses the Generative Pre-trained Transformer 2 (GPT-2) model by [OpenAI](https://openai.com/). This is a precursor of GPT-3.5, the model used for ChatGPT. This model was released in 2019 and you can find more information by reading its [model card](https://huggingface.co/gpt2/tree/main) on the 🤗 Transformers website. We include here several parameters:\n",
    "\n",
    "* `set_seed` Remove the randomness of the text generation by supplying the same seed value each time.\n",
    "* `prompt` The prompt that the text generator uses to build the sequence.\n",
    "* `max_length` The length of the text returned. More text requires more time and the limit is defined by the model.\n",
    "* `num_return_sequences` Allows more than one sequence to be returned for the prompt."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bb0d0-bd68-4ca9-9fdd-a8b809ae1ac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Text Generation\n",
    "input_text = \"The Legend of Zelda: Tears of the Kingdom is a video game that\"\n",
    "\n",
    "generator = pipeline('text-generation', model='gpt2')\n",
    "\n",
    "def create_text(prompt):\n",
    "    #set_seed(42)\n",
    "    return generator(prompt, max_length=100, num_return_sequences=3)\n",
    "\n",
    "create_text(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32bb8929-b2f3-4d3d-976c-c2513135a84b",
   "metadata": {},
   "source": [
    "# Sentiment Analysis\n",
    "\n",
    "By default, the 🤗 Transformers library text-classification pipeline uses the [distilbert-base-uncased-finetuned-sst-2-english](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) model. This model is based on a distilled, uncased version of [BERT](https://huggingface.co/bert-base-uncased) that has been fine-tuned on the [Stanford Sentiment Treebank 2](https://huggingface.co/datasets/sst2) (SST-2) dataset. The SST-2 dataset is a binary classification dataset for training models to learn the sentiment of words, phrases, and sentences. It contains 215,154 unique manually labeled texts of varying lengths. The model card describes SST-2:\n",
    "\n",
    ">\n",
    "The corpus is based on the dataset introduced by Pang and Lee (2005) and consists of 11,855 single sentences extracted from movie reviews. It was parsed with the Stanford parser and includes a total of 215,154 unique phrases from those parse trees, each annotated by 3 human judges.\n",
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4fdf6b-b3d1-44e6-96fa-5d84f5f71574",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis Prompts\n",
    "\n",
    "prompt1 = \"\"\"\n",
    "I really wanted to like this game as I enjoyed the first one on the switch. Problem is, tears is FAR more difficult than the original. It’s not a casual game at all anymore. It takes far too much time, effort, research (online) to figure out where to find hidden areas and there are lots of “stuck points” (dead ends) that have no way of getting out except going to previous game save.\n",
    "\n",
    "Pros:\n",
    "1. Beautiful graphics and environments\n",
    "2. Mending/Attaching, Ascending\n",
    "3. Dynamic loading of zones (developers should be commended)\n",
    "\n",
    "Cons:\n",
    "1. The controls are lazy. The jump button is at the top, sprint is where jump should be. Attack is on opposite of where every other game is. It’s equivalent of having one App on an iPhone that doesn’t use basic swipe functionality. Quite frankly, it’s bad design\n",
    "2. The first Zelda showed what/where you’re supposed to do/get to in a shrine via a quick cut scene. This one does none of that. What’s worse, is that on the sky island, you’re actually given a new power and then if you leave (because the puzzle is obscenely hidden) - you don’t actually have the power. Puzzles are 10x’s harder. Example of nearly impossible puzzles use the rewind power (talk about next level difficulty: there’s a two clock handle gate puzzle that took me two days to get through. One puzzle!)\n",
    "3. Far more fighting/combat. Takes away the fun experience of exploring and talking to people doing side quests. And combat is just far more difficult in this game. The first robot shrine to train you, is just hard/horrible for a first tutorial.\n",
    "4. Sky island (first zone) is just overall at a ridiculously high level of difficulty. I got so disappointed (too much running around and end areas/or falling) that I abandoned the game for a few weeks and eventually came back. I had to watch several YouTube videos to figure out where the 4th shrine entrance was in a dark cave.\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "prompt2 = \"\"\"\n",
    "Where to begin with this game? Did you enjoy Breath of the Wild? Were you frustrated by certain aspects? If the answer was yes to both, you’ll love this game.\n",
    "\n",
    "I have been a Zelda fan all the way back to playing Zelda 2 when I was 4. And when Breath of the Wild came out, I was blown away, but also frustrated by aspects of it. Yes the game did away with most of the standard Zelda formula, but enough still remained for me to enjoy it as a Zelda game. What blew me away though was the size of the game, the focus on exploration and many other things. However, many things frustrated me like how long it took to get around, warping to shrines was easy, but losing your horse was always a serious pain.\n",
    "\n",
    "Now we arrive at the new game Tears of the Kingdom, clearly a sequel to BOTW that uses the same map, but manages to keep it fresh by introducing sky islands and the depths. And first off, if you felt BOTW had a massive map, this will blow you away. There is so much more to explore, but yet the game moves SO much faster by changing the towers from things you have to slowly climb to activate to just making them usually have a small puzzle to get them activated. Once you have these towers up, the game gets MUCH easier to navigate as each launches you miles into the sky where you can glide down toward the next closest one, or perhaps a shrine or other notable area. This eliminated the problem I had with horses and in fact, I barely used mine over the course of the entire game.\n",
    "\n",
    "The new powers you gain at the start also open this up to a realm of creativity previously offered by other games like Minecraft, but in a Zelda game, it feels fresh. I can only say this game will feel like a dream to any engineers or someone with an interest in building. You can truly get lost in crafting weapons or vehicles, but even if only done when necessary it’s a lot of fun. I who think the shrine puzzles have gotten much harder (at least for someone like me), but they were still fun and brilliant to experience.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64cdd0f1-4e47-4d5a-b879-aafabec29d45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Sentiment Analysis Pipeline\n",
    "classifier = pipeline(\"text-classification\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c553196f-fda6-47db-85d5-506b1d02f629",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def classify_sentiment(prompt):\n",
    "        output = classifier(prompt)\n",
    "        return output\n",
    "\n",
    "classify_sentiment(prompt2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1ccb6a9-45ff-41bc-a1ab-e8225fc7e041",
   "metadata": {},
   "source": [
    "# Named Entity Recognition\n",
    "\n",
    "The `aggregation_strategy` parameter defines the strategy used to group entity tokens together, like \"New York\". Remember, the tokenization may also be at the subword level, so you could see \"Microsoft\" broken up into \"Micro\" and \"soft\". Additional aggregration strategies such as \"first\", \"average\", and \"max\" are discussed in the 🤗 Transformers [documentation](https://huggingface.co/transformers/v4.7.0/_modules/transformers/pipelines/token_classification.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a95b8c-a1db-41fd-8f58-d10497755967",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Named Entity Recognition Pipeline\n",
    "ner_tagger = pipeline(\"ner\", aggregation_strategy=\"simple\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06f69d8-3e28-4847-a7e0-2e718d4f626c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_entities(prompt):\n",
    "    output = ner_tagger(prompt)\n",
    "    return pd.DataFrame(output)\n",
    "\n",
    "extract_entities(prompt2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "037df519-beab-48fd-816b-da277f4da864",
   "metadata": {},
   "source": [
    "# Question Answering\n",
    "\n",
    "The Question Answering pipeline has two required parameters: \n",
    "\n",
    "* `question` The question being asked\n",
    "* `context` The source material that should be used to answer this question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a8a459-f47e-4771-8867-a696bc30b8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Question Answering Pipeline\n",
    "reader = pipeline(\"question-answering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34fbd425-4e64-4105-9561-9f7186e23bcf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "question = \"What are the best parts of Tears of the Kingdom?\"\n",
    "\n",
    "def answer_question(question):\n",
    "    output = reader(question=question, context=prompt2)\n",
    "    return pd.DataFrame([output])\n",
    "\n",
    "answer_question(question)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e585e817-dae4-44a2-a9e8-20603c2361bc",
   "metadata": {},
   "source": [
    "# Summarization\n",
    "\n",
    "The `clean_up_tokenization_spaces` parameter removes extraneous spaces created through the detokenization process. If tokenization breaks up a string into separate tokens, then detokenization joins together a series of tokens into a string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a167b118-95ff-4d95-b2ab-7f7d755a9b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Summarization pipeline\n",
    "summarizer = pipeline(\"summarization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5363750c-93eb-4bae-99f3-f13c1a055e06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def summarize(text):\n",
    "    outputs = summarizer(text, max_length=75, clean_up_tokenization_spaces=True)\n",
    "    return outputs[0]['summary_text']\n",
    "\n",
    "print(summarize(prompt2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d242b9a4-dcb3-491b-abaa-fd8100235a73",
   "metadata": {},
   "source": [
    "# Translation\n",
    "\n",
    "The translation pipeline may have length limitations based on the model selected. If your text is long, you may need to break it up into smaller chunks for analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b32c484-a641-4dc4-8987-409471734443",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "chunk1 = prompt2[:952]\n",
    "chunk2 = prompt2[952:]\n",
    "\n",
    "translator = pipeline('translation_en_to_de', model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(chunk1, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(chunk1 + '\\n')\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8376b9a6-2041-4881-ac80-7ac3ad2d82d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "translator = pipeline('translation_en_to_de', model=\"Helsinki-NLP/opus-mt-en-de\")\n",
    "outputs = translator(chunk2, clean_up_tokenization_spaces=True, min_length=100)\n",
    "print(chunk2 + '\\n')\n",
    "print(outputs[0]['translation_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378ef337-d6de-442c-b760-e62e86fa01d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the .cache folder\n",
    "!rm -r /home/jovyan/.cache/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
